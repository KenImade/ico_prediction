data <- read.csv("Data/data_preprocessed.csv")
# Factoring target variable
data$success <- factor(data$success, levels = c("No", "Yes"))
table(data$success)
# splitting data
training_indices <- createDataPartition(data$success, p = 0.8,
list = FALSE)
train_data <- data[training_indices,]
test_data <- data[-training_indices,]
train_data <- train_data[,1:11]
test_data <- test_data[,1:11]
# Naive Bayes ----
nb_model <- train(success ~., data = train_data, method = "naive_bayes",
trControl = trainControl(method = "cv", number = 10))
# Naive Bayes ----
nb_model <- train(success ~., data = train_data, method = "naive_bayes",
trControl = trainControl(method = "cv", number = 10))
nb_predict <- predict(nb_model, newdata = test_data)
CrossTable(nb_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
accuracy <- function(actual, predicted) {
correct <- sum(actual == predicted)
total <- length(actual)
accuracy <- correct / total
return(accuracy)
}
precision <- function(actual, predicted, positive_label) {
true_positive <- sum(actual == positive_label & predicted == positive_label)
false_positive <- sum(actual != positive_label & predicted == positive_label)
precision <- true_positive / (true_positive + false_positive)
return(precision)
}
nb_accuracy <- accuracy(test_data$success, nb_predict)
nb_precision <- precision(test_data$success, nb_predict, "Yes")
nb_sensitivity <- sensitivity(nb_predict, test_data$success, positive = "Yes")
nb_specificity <- specificity(nb_predict, test_data$success, negative = "No")
ctrl_1 <- trainControl(method = "cv", number = 10)
ctrl_2 <- trainControl(method = "CV", number = 30)
ctrl_3 <- trainControl(method = "CV", number = 50)
set.seed(128)
# Decision Trees ----
dt_model <- train(succes ~., data = train_data, method = "rpart",
trControl = ctrl_1)
# Decision Trees ----
dt_model <- train(success ~., data = train_data, method = "rpart",
trControl = ctrl_1)
dt_predict <- predict(dt_model, newdata = test_data)
CrossTable(dt_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
dt_accuracy <- accuracy(test_data$success, dt_predict)
dt_precision <- precision(test_data$success, dt_predict, "Yes")
dt_sensitivity <- sensitivity(dt_predict, test_data$success, positive = "Yes")
dt_specificity <- specificity(dt_predict, test_data$success, negative = "No")
# SVM ----
svm_model <- train(success ~., data = train_data, method = "svmRadial",
trainControl = ctrl_1)
svm_predict <- predict(svm_model, newdata = test_data)
CrossTable(svm_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
svm_accuracy <- accuracy(test_data$success, svm_predict)
svm_precision <- precision(test_data$success, svm_predict, "Yes")
svm_sensitivity <- sensitivity(svm_predict, test_data$success, positive = "Yes")
svm_specificity <- specificity(svm_predict, test_data$success, negative = "No")
# ANN ----
ann_model <- train(success ~., data = train_data, method = "nnet",
trainControl = ctrl_1)
ann_predict <- predict(ann_model, newdata = test_data)
CrossTable(ann_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
ann_predict
ann_accuracy <- accuracy(test_data$success, ann_predict)
ann_precision <- precision(test_data$success, ann_predict, "Yes")
View(ann_model)
ann_predict <- predict(ann_model, newdata = select(test_data, -success))
# Convert ann_predict to binary labels
ann_predict_labels <- apply(ann_predict, 1, function(row) {
ifelse(row[1] > row[2], "No", "Yes")
})
CrossTable(ann_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
?neuralnet
library(neuralnet)
?neuralnet
ann_accuracy <- accuracy(test_data$success, ann_predict)
ann_precision <- precision(test_data$success, ann_predict, "Yes")
ann_sensitivity <- sensitivity(ann_predict, test_data$success, positive = "Yes")
ann_specificity <- specificity(ann_predict, test_data$success, negative = "No")
# Calculate accuracy
ann_accuracy <- confusionMatrix(ann_predict, test_data$success)$overall['Accuracy']
# Calculate precision
ann_precision <- confusionMatrix(ann_predict, test_data$success)$byClass['Positive_Prediction_Value']
CrossTable(ann_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
ann_predict
x <- as.data.frame(ann_predict)
View(x)
summary(x$ann_predict)
# ANN ----
# training
ann_model <- neuralnet(formula = success ~., data = train_data,
trainControl = ctrl_1)
# ANN ----
# training
ann_model <- neuralnet(formula = success ~., data = train_data)
# obtain predicted values
ann_predict <- predict(ann_model, select(test_data, -success))
# Convert ann_predict to binary labels
ann_predict_labels <- apply(ann_predict, 1, function(row) {
ifelse(row[1] > row[2], "No", "Yes")
})
CrossTable(ann_predict_labels, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
# Logistic Regression ----
# training
lr_model <- train(success ~ ., data = train_data, method = "glm",
family = "binomial", trControl = ctrl_1)
# obtain predicted values
lr_predict <- predict(lr_model, test_data)
CrossTable(lr_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
lr_accuracy <- accuracy(test_data$success, lr_predict)
lr_precision <- precision(test_data$success, lr_predict, "Yes")
lr_sensitivity <- sensitivity(lr_predict, test_data$success, positive = "Yes")
lr_specificity <- specificity(lr_predict, test_data$success, negative = "No")
# DATA MODELLING ----
set.seed(128)
# libraries
library(dplyr)
library(caret)
library(gmodels)
library(cluster)
library(ggplot2)
library(pROC)
library(ROCR)
# load dataset
data <- read.csv("Data/data_preprocessed.csv")
# Factoring target variable
data$success <- factor(data$success, levels = c("No", "Yes"))
table(data$success)
# splitting data
training_indices <- createDataPartition(data$success, p = 0.8,
list = FALSE)
train_data <- data[training_indices,]
test_data <- data[-training_indices,]
train_data <- train_data[,1:11]
test_data <- test_data[,1:11]
summary(train_data[,1:11])
# Measures ----
accuracy <- function(actual, predicted) {
correct <- sum(actual == predicted)
total <- length(actual)
accuracy <- correct / total
return(accuracy)
}
precision <- function(actual, predicted, positive_label) {
true_positive <- sum(actual == positive_label & predicted == positive_label)
false_positive <- sum(actual != positive_label & predicted == positive_label)
precision <- true_positive / (true_positive + false_positive)
return(precision)
}
# Controls
ctrl_1 <- trainControl(method = "cv", number = 10)
ctrl_2 <- trainControl(method = "CV", number = 30)
ctrl_3 <- trainControl(method = "CV", number = 50)
# KNN ----
knn_model <- train(success ~ .,
data = train_data,
method = "knn",
trControl = ctrl_1,
preProcess = c("center", "scale"))
knn_predict <- predict(knn_model, newdata = test_data)
CrossTable(test_data$success, knn_predict, dnn=c('actual', 'predict'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
knn_accuracy <- accuracy(test_data$success, knn_predict)
knn_precision <- precision(test_data$success, knn_predict, "Yes")
knn_sensitivity <- sensitivity(knn_predict, test_data$success, positive = "Yes")
knn_specificity <- specificity(knn_predict, test_data$success, negative = "No")
# Naive Bayes ----
nb_model <- train(success ~., data = train_data, method = "naive_bayes",
trControl = ctrl_1)
nb_predict <- predict(nb_model, newdata = test_data)
CrossTable(nb_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
nb_accuracy <- accuracy(test_data$success, nb_predict)
nb_precision <- precision(test_data$success, nb_predict, "Yes")
nb_sensitivity <- sensitivity(nb_predict, test_data$success, positive = "Yes")
nb_specificity <- specificity(nb_predict, test_data$success, negative = "No")
# Decision Trees ----
dt_model <- train(success ~., data = train_data, method = "rpart",
trControl = ctrl_1)
dt_predict <- predict(dt_model, newdata = test_data)
CrossTable(dt_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
dt_accuracy <- accuracy(test_data$success, dt_predict)
dt_precision <- precision(test_data$success, dt_predict, "Yes")
dt_sensitivity <- sensitivity(dt_predict, test_data$success, positive = "Yes")
dt_specificity <- specificity(dt_predict, test_data$success, negative = "No")
# SVM ----
svm_model <- train(success ~., data = train_data, method = "svmRadial",
trainControl = ctrl_1)
svm_predict <- predict(svm_model, newdata = test_data)
CrossTable(svm_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
svm_accuracy <- accuracy(test_data$success, svm_predict)
svm_precision <- precision(test_data$success, svm_predict, "Yes")
svm_sensitivity <- sensitivity(svm_predict, test_data$success, positive = "Yes")
svm_specificity <- specificity(svm_predict, test_data$success, negative = "No")
# Logistic Regression ----
# training
lr_model <- train(success ~ ., data = train_data, method = "glm",
family = "binomial", trControl = ctrl_1)
# obtain predicted values
lr_predict <- predict(lr_model, test_data)
CrossTable(lr_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
lr_accuracy <- accuracy(test_data$success, lr_predict)
lr_precision <- precision(test_data$success, lr_predict, "Yes")
lr_sensitivity <- sensitivity(lr_predict, test_data$success, positive = "Yes")
lr_specificity <- specificity(lr_predict, test_data$success, negative = "No")
?prediction
# ROC Curve
knn_object <- prediction(knn_predict, test_data$success)
# ROC Curve
knn_object <- prediction(knn_predict, test_data$success)
# ROC Curve
knn_predict_probs <- predict(knn_model, newdata = test_data, type = "prob")
# Calculate the ROC curve
roc_curve <- roc(test_data$success, knn_predict_probs[, "Yes"])
# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
ylab = "True Positive Rate")
nb_roc_curve <- roc(test_data$success, nb_predict_probs[, "Yes"])
nb_predict_probs <- predict(nb_model, newdata = test_data, type = "prob")
nb_roc_curve <- roc(test_data$success, nb_predict_probs[, "Yes"])
# Plot the ROC curve
plot(knn_roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
ylab = "True Positive Rate", col = "blue")
# Calculate the ROC curve
knn_roc_curve <- roc(test_data$success, knn_predict_probs[, "Yes"])
# Plot the ROC curve
plot(knn_roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
ylab = "True Positive Rate", col = "blue")
plot(nb_roc_curve, col = "red", add = TRUE, print.auc = TRUE, print.auc.y = 0.4)
legend("bottomright", legend = c("KNN Model", "Naive Bayes Model"),
col = c("blue", "red"), lty = 1, bty = "n")
plot(nb_roc_curve, col = "red", add = TRUE, print.auc = TRUE, print.auc.y = 0.2)
# Plot the ROC curve
plot(knn_roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
ylab = "True Positive Rate", col = "blue")
plot(nb_roc_curve, col = "red", add = TRUE, print.auc = TRUE)
legend("bottomright", legend = c("KNN Model", "Naive Bayes Model"),
col = c("blue", "red"), lty = 1, bty = "n")
plot(nb_roc_curve, col = "red", add = TRUE)
dt_predict_probs <- predict(dt_model, newdata = test_data, type = "prob")
dt_roc_curve <- roc(test_data$success, dt_predict_probs[, "Yes"])
plot(dt_roc_curve, col = "yellow", add = TRUE)
legend("bottomright", legend = c("KNN Model", "Naive Bayes Model",
"Decision Trees"),
col = c("blue", "red", "yellow"), lty = 1, bty = "n")
svm_predict_probs <- predict(svm_model, newdata = test_data, type = "prob")
svm_roc_curve <- roc(test_data$success, svm_predict_probs[, "Yes"])
View(svm_predict_probs)
lr_predict_probs <- predict(lr_model, newdata = test_data, type = "prob")
lr_roc_curve <- roc(test_data$success, lr_predict_probs[, "Yes"])
plot(lr_roc_curve, col = "green", add = TRUE)
legend("bottomright", legend = c("KNN Model", "Naive Bayes Model",
"Decision Trees", "Logistic Regression"),
col = c("blue", "red", "yellow", "green"), lty = 1, bty = "n")
legend("right", c("KNN Model", "Naive Bayes Model", "Decision Trees",
"Logistic Regression"),
col = c("blue", "red", "yellow", "green"), lty = 1, bty = "n")
legend("right", c("KNN Model", "Naive Bayes Model", "Decision Trees",
"Logistic Regression"),
col = c("blue", "red", "yellow", "green"), lty = 1, bty = "n")
lot(knn_roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
ylab = "True Positive Rate", col = "blue")
plot(knn_roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
ylab = "True Positive Rate", col = "blue")
plot(nb_roc_curve, col = "red", add = TRUE, print.auc = TRUE)
plot(dt_roc_curve, col = "yellow", add = TRUE)
plot(lr_roc_curve, col = "green", add = TRUE)
legend("right", c("KNN Model", "Naive Bayes Model", "Decision Trees",
"Logistic Regression"),
col = c("blue", "red", "yellow", "green"), lty = 1, bty = "n")
# DATA MODELLING ----
set.seed(128)
# libraries
library(dplyr)
library(caret)
library(gmodels)
library(cluster)
library(pROC)
library(ROCR)
# load dataset
data <- read.csv("Data/data_preprocessed.csv")
# Factoring target variable
data$success <- factor(data$success, levels = c("No", "Yes"))
table(data$success)
# splitting data
training_indices <- createDataPartition(data$success, p = 0.8,
list = FALSE)
train_data <- data[training_indices,]
test_data <- data[-training_indices,]
train_data <- train_data[,1:11]
test_data <- test_data[,1:11]
summary(train_data[,1:11])
# Measures ----
accuracy <- function(actual, predicted) {
correct <- sum(actual == predicted)
total <- length(actual)
accuracy <- correct / total
return(accuracy)
}
precision <- function(actual, predicted, positive_label) {
true_positive <- sum(actual == positive_label & predicted == positive_label)
false_positive <- sum(actual != positive_label & predicted == positive_label)
precision <- true_positive / (true_positive + false_positive)
return(precision)
}
# Controls
ctrl_1 <- trainControl(method = "cv", number = 10)
ctrl_2 <- trainControl(method = "CV", number = 30)
ctrl_3 <- trainControl(method = "CV", number = 50)
# KNN ----
knn_model <- train(success ~ .,
data = train_data,
method = "knn",
trControl = ctrl_1,
preProcess = c("center", "scale"))
knn_predict <- predict(knn_model, newdata = test_data)
CrossTable(test_data$success, knn_predict, dnn=c('actual', 'predict'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
knn_accuracy <- accuracy(test_data$success, knn_predict)
knn_precision <- precision(test_data$success, knn_predict, "Yes")
knn_sensitivity <- sensitivity(knn_predict, test_data$success, positive = "Yes")
knn_specificity <- specificity(knn_predict, test_data$success, negative = "No")
# Naive Bayes ----
nb_model <- train(success ~., data = train_data, method = "naive_bayes",
trControl = ctrl_1)
nb_predict <- predict(nb_model, newdata = test_data)
CrossTable(nb_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
nb_accuracy <- accuracy(test_data$success, nb_predict)
nb_precision <- precision(test_data$success, nb_predict, "Yes")
nb_sensitivity <- sensitivity(nb_predict, test_data$success, positive = "Yes")
nb_specificity <- specificity(nb_predict, test_data$success, negative = "No")
# Decision Trees ----
dt_model <- train(success ~., data = train_data, method = "rpart",
trControl = ctrl_1)
dt_predict <- predict(dt_model, newdata = test_data)
CrossTable(dt_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
dt_accuracy <- accuracy(test_data$success, dt_predict)
dt_precision <- precision(test_data$success, dt_predict, "Yes")
dt_sensitivity <- sensitivity(dt_predict, test_data$success, positive = "Yes")
dt_specificity <- specificity(dt_predict, test_data$success, negative = "No")
# SVM ----
svm_model <- train(success ~., data = train_data, method = "svmRadial",
trainControl = ctrl_1)
svm_predict <- predict(svm_model, newdata = test_data)
CrossTable(svm_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
svm_accuracy <- accuracy(test_data$success, svm_predict)
svm_precision <- precision(test_data$success, svm_predict, "Yes")
svm_sensitivity <- sensitivity(svm_predict, test_data$success, positive = "Yes")
svm_specificity <- specificity(svm_predict, test_data$success, negative = "No")
# Logistic Regression ----
# training
lr_model <- train(success ~ ., data = train_data, method = "glm",
family = "binomial", trControl = ctrl_1)
# obtain predicted values
lr_predict <- predict(lr_model, test_data)
CrossTable(lr_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
lr_accuracy <- accuracy(test_data$success, lr_predict)
lr_precision <- precision(test_data$success, lr_predict, "Yes")
lr_sensitivity <- sensitivity(lr_predict, test_data$success, positive = "Yes")
lr_specificity <- specificity(lr_predict, test_data$success, negative = "No")
# # ROC Curve
# knn_predict_probs <- predict(knn_model, newdata = test_data, type = "prob")
# nb_predict_probs <- predict(nb_model, newdata = test_data, type = "prob")
# dt_predict_probs <- predict(dt_model, newdata = test_data, type = "prob")
# svm_predict_probs <- predict(svm_model, newdata = test_data, type = "prob")
# lr_predict_probs <- predict(lr_model, newdata = test_data, type = "prob")
#
# # Calculate the ROC curve
# knn_roc_curve <- roc(test_data$success, knn_predict_probs[, "Yes"])
# nb_roc_curve <- roc(test_data$success, nb_predict_probs[, "Yes"])
# dt_roc_curve <- roc(test_data$success, dt_predict_probs[, "Yes"])
# # svm_roc_curve <- roc(test_data$success, svm_predict_probs[, "Yes"])
# lr_roc_curve <- roc(test_data$success, lr_predict_probs[, "Yes"])
#
# # Plot the ROC curve
# plot(knn_roc_curve, main = "ROC Curve", xlab = "False Positive Rate",
#      ylab = "True Positive Rate", col = "blue")
# plot(nb_roc_curve, col = "red", add = TRUE, print.auc = TRUE)
# plot(dt_roc_curve, col = "yellow", add = TRUE)
# plot(lr_roc_curve, col = "green", add = TRUE)
#
# legend("right", c("KNN Model", "Naive Bayes Model", "Decision Trees",
#                   "Logistic Regression"),
#        col = c("blue", "red", "yellow", "green"), lty = 1, bty = "n")
# KNN ----
knn_model <- train(success ~ .,
data = train_data,
method = "knn",
trControl = ctrl_2,
preProcess = c("center", "scale"))
knn_predict <- predict(knn_model, newdata = test_data)
CrossTable(test_data$success, knn_predict, dnn=c('actual', 'predict'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
knn_accuracy <- accuracy(test_data$success, knn_predict)
knn_precision <- precision(test_data$success, knn_predict, "Yes")
knn_sensitivity <- sensitivity(knn_predict, test_data$success, positive = "Yes")
knn_specificity <- specificity(knn_predict, test_data$success, negative = "No")
# Naive Bayes ----
nb_model <- train(success ~., data = train_data, method = "naive_bayes",
trControl = ctrl_2)
nb_predict <- predict(nb_model, newdata = test_data)
CrossTable(nb_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
nb_accuracy <- accuracy(test_data$success, nb_predict)
nb_precision <- precision(test_data$success, nb_predict, "Yes")
nb_sensitivity <- sensitivity(nb_predict, test_data$success, positive = "Yes")
nb_specificity <- specificity(nb_predict, test_data$success, negative = "No")
# Decision Trees ----
dt_model <- train(success ~., data = train_data, method = "rpart",
trControl = ctrl_2)
dt_predict <- predict(dt_model, newdata = test_data)
CrossTable(dt_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
dt_accuracy <- accuracy(test_data$success, dt_predict)
dt_precision <- precision(test_data$success, dt_predict, "Yes")
dt_sensitivity <- sensitivity(dt_predict, test_data$success, positive = "Yes")
dt_specificity <- specificity(dt_predict, test_data$success, negative = "No")
# SVM ----
svm_model <- train(success ~., data = train_data, method = "svmRadial",
trainControl = ctrl_2)
svm_predict <- predict(svm_model, newdata = test_data)
CrossTable(svm_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
svm_accuracy <- accuracy(test_data$success, svm_predict)
svm_precision <- precision(test_data$success, svm_predict, "Yes")
svm_sensitivity <- sensitivity(svm_predict, test_data$success, positive = "Yes")
svm_specificity <- specificity(svm_predict, test_data$success, negative = "No")
# Logistic Regression ----
# training
lr_model <- train(success ~ ., data = train_data, method = "glm",
family = "binomial", trControl = ctrl_2)
# obtain predicted values
lr_predict <- predict(lr_model, test_data)
CrossTable(lr_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
lr_accuracy <- accuracy(test_data$success, lr_predict)
lr_precision <- precision(test_data$success, lr_predict, "Yes")
lr_sensitivity <- sensitivity(lr_predict, test_data$success, positive = "Yes")
lr_specificity <- specificity(lr_predict, test_data$success, negative = "No")
# KNN ----
knn_model <- train(success ~ .,
data = train_data,
method = "knn",
trControl = ctrl_3,
preProcess = c("center", "scale"))
knn_predict <- predict(knn_model, newdata = test_data)
CrossTable(test_data$success, knn_predict, dnn=c('actual', 'predict'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
knn_accuracy <- accuracy(test_data$success, knn_predict)
knn_precision <- precision(test_data$success, knn_predict, "Yes")
knn_sensitivity <- sensitivity(knn_predict, test_data$success, positive = "Yes")
knn_specificity <- specificity(knn_predict, test_data$success, negative = "No")
# Naive Bayes ----
nb_model <- train(success ~., data = train_data, method = "naive_bayes",
trControl = ctrl_3)
nb_predict <- predict(nb_model, newdata = test_data)
CrossTable(nb_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
nb_accuracy <- accuracy(test_data$success, nb_predict)
nb_precision <- precision(test_data$success, nb_predict, "Yes")
nb_sensitivity <- sensitivity(nb_predict, test_data$success, positive = "Yes")
nb_specificity <- specificity(nb_predict, test_data$success, negative = "No")
# Decision Trees ----
dt_model <- train(success ~., data = train_data, method = "rpart",
trControl = ctrl_3)
dt_predict <- predict(dt_model, newdata = test_data)
CrossTable(dt_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
dt_accuracy <- accuracy(test_data$success, dt_predict)
dt_precision <- precision(test_data$success, dt_predict, "Yes")
dt_sensitivity <- sensitivity(dt_predict, test_data$success, positive = "Yes")
dt_specificity <- specificity(dt_predict, test_data$success, negative = "No")
# SVM ----
svm_model <- train(success ~., data = train_data, method = "svmRadial",
trainControl = ctrl_3)
svm_predict <- predict(svm_model, newdata = test_data)
CrossTable(svm_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
svm_accuracy <- accuracy(test_data$success, svm_predict)
svm_precision <- precision(test_data$success, svm_predict, "Yes")
svm_sensitivity <- sensitivity(svm_predict, test_data$success, positive = "Yes")
svm_specificity <- specificity(svm_predict, test_data$success, negative = "No")
# Logistic Regression ----
# training
lr_model <- train(success ~ ., data = train_data, method = "glm",
family = "binomial", trControl = ctrl_3)
# obtain predicted values
lr_predict <- predict(lr_model, test_data)
CrossTable(lr_predict, test_data$success, dnn=c('predict', 'actual'),
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE)
lr_accuracy <- accuracy(test_data$success, lr_predict)
lr_precision <- precision(test_data$success, lr_predict, "Yes")
lr_sensitivity <- sensitivity(lr_predict, test_data$success, positive = "Yes")
lr_specificity <- specificity(lr_predict, test_data$success, negative = "No")
